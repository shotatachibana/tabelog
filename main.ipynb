{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7536cf-ad9f-4eec-ab56-96b048ef4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "class TownUrlExtractor:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    def fetch_html(self, url):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def extract_towns(self, html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        towns = []\n",
    "        \n",
    "        panels = soup.find('div', class_='c-tabs__panels list-balloon__towns')\n",
    "        if panels:\n",
    "            links = panels.find_all('a', class_='c-link-arrow')\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                span = link.find('span')\n",
    "                town_name = span.text.strip() if span else ''\n",
    "                if href and town_name:\n",
    "                    towns.append({'town_name': town_name, 'url': href})\n",
    "        \n",
    "        return towns\n",
    "    \n",
    "    def process_dataframe(self, df, url_column=None):\n",
    "        \"\"\"\n",
    "        DataFrameを処理してタウン情報を抽出する\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): 処理対象のDataFrame\n",
    "            url_column (str, optional): URLが含まれるカラム名\n",
    "        \n",
    "        Returns:\n",
    "            list: 抽出されたタウン情報のリスト\n",
    "        \"\"\"\n",
    "        # URLカラムの自動検出\n",
    "        if url_column is None:\n",
    "            for col in df.columns:\n",
    "                if 'url' in col.lower() or 'link' in col.lower():\n",
    "                    url_column = col\n",
    "                    break\n",
    "            if url_column is None:\n",
    "                url_column = df.columns[0]\n",
    "        \n",
    "        all_towns = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            url = row[url_column]\n",
    "            if pd.notna(url):  # NaNチェックを追加\n",
    "                html_content = self.fetch_html(url)\n",
    "                if html_content:\n",
    "                    towns = self.extract_towns(html_content)\n",
    "                    all_towns.extend(towns)\n",
    "        \n",
    "        return all_towns\n",
    "    \n",
    "    def save_results(self, towns, output_file='extracted_towns.csv'):\n",
    "        \"\"\"\n",
    "        結果をCSVファイルに保存する\n",
    "        \n",
    "        Args:\n",
    "            towns (list): タウン情報のリスト\n",
    "            output_file (str): 出力ファイル名\n",
    "        \n",
    "        Returns:\n",
    "            str: 出力ファイル名\n",
    "        \"\"\"\n",
    "        pd.DataFrame(towns).to_csv(output_file, index=False, encoding='utf-8')\n",
    "        return output_file\n",
    "\n",
    "class TabelogStoreScraper:\n",
    "    \"\"\"\n",
    "    食べログから店舗詳細情報を取得するクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, url_list=None, max_num=60):\n",
    "        \"\"\"\n",
    "        店舗詳細情報のスクレイピングを開始する\n",
    "        \n",
    "        Args:\n",
    "            url_list (list): URLのリスト。文字列のリストまたは辞書のリスト。Noneの場合は元のデフォルトURL\n",
    "            max_num (int): 各URLで検索するページ上限数\n",
    "        \"\"\"\n",
    "        # 取得したデータ一覧の保存場所\n",
    "        self.RESULT_PATH = 'out'\n",
    "        # 取得したデータ一覧名\n",
    "        self.STORE_RESULT_NAME = 'store_details.csv'\n",
    "        # 店舗詳細データフレームの列名（地図情報を追加）\n",
    "        self.STORE_COLUMNS = ['store_id', 'store_name', 'genre', 'rate', 'review_cnt', 'store_url',\n",
    "                             'address', 'latitude', 'longitude', 'business_hours', 'reserve', \n",
    "                             'dinner_budget', 'lunch_budget', 'opened']\n",
    "        # 検索するページ上限数(1,200[20*60]件以降は表示不可)\n",
    "        self.max_num = max_num + 1 \n",
    "        self.shop_id = ''\n",
    "        self.id_num = 1\n",
    "        self.store_df = pd.DataFrame(columns=self.STORE_COLUMNS)\n",
    "        \n",
    "        # URLリストの設定\n",
    "        if url_list:\n",
    "            # 辞書のリストの場合はURLを抽出\n",
    "            if isinstance(url_list[0], dict):\n",
    "                self.url_list = [item['url'] for item in url_list]\n",
    "            else:\n",
    "                self.url_list = url_list\n",
    "        else:\n",
    "            # デフォルトのURL（元のコードと同じ）\n",
    "            self.url_list = ['https://tabelog.com/tokyo/C13105/C36312/rstLst/']\n",
    "\n",
    "        self._scrape_store_details()\n",
    "\n",
    "    def _scrape_store_details(self):\n",
    "        \"\"\"\n",
    "        店舗詳細情報をスクレイピングするメソッド\n",
    "        \"\"\"\n",
    "        try:\n",
    "            first_time = time.time()\n",
    "            \n",
    "            # 各URLに対して処理を実行\n",
    "            for base_url in self.url_list:\n",
    "                for page_num in range(1, self.max_num):\n",
    "                    # ページ番号をURLに追加\n",
    "                    if base_url.endswith('/'):\n",
    "                        page_url = base_url + str(page_num) + '/?Srt=D&SrtT=rt&sort_mode=1'\n",
    "                    else:\n",
    "                        page_url = base_url + '/' + str(page_num) + '/?Srt=D&SrtT=rt&sort_mode=1'\n",
    "\n",
    "                    response = self.connect_url(page_url)\n",
    "                    # HTML解析用の変数\n",
    "                    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                    # 現ページの表示件数\n",
    "                    count_num_element = soup.find('span', class_='c-page-count__num')\n",
    "                    count_num = count_num_element.get_text(strip=True) if count_num_element else '0'\n",
    "                    # 0件の場合はbreak\n",
    "                    if count_num == '0':\n",
    "                        break\n",
    "                    \n",
    "                    # 一覧ページから以下の店舗情報を取得するリスト化 \n",
    "                    # ジャンルの取得\n",
    "                    genre_info = [genre.text.rstrip() for genre in soup.find_all('div', class_='list-rst__area-genre cpy-area-genre')]\n",
    "                    # 評価点の取得\n",
    "                    rate_list = [float(rate.text) for rate in soup.find_all('span', class_='c-rating__val c-rating__val--strong list-rst__rating-val')]\n",
    "                    # 口コミ件数の取得\n",
    "                    count_list = [count.text for count in soup.find_all('em', class_='list-rst__rvw-count-num cpy-review-count')]\n",
    "                    # 店名の取得\n",
    "                    raw_data = soup.find_all('a', class_='list-rst__rst-name-target cpy-rst-name js-ranking-num')\n",
    "                    shop_list = [name.text for name in raw_data]\n",
    "                    shop_url_list = [shop_url.get('href') for shop_url in raw_data]\n",
    "\n",
    "                    # 最大のリスト長を取得\n",
    "                    max_length = max(len(shop_list), len(genre_info), len(rate_list), len(count_list), len(shop_url_list))\n",
    "                    \n",
    "                    # 各リストを最大長に合わせて空文字で埋める\n",
    "                    while len(genre_info) < max_length:\n",
    "                        genre_info.append(\"\")\n",
    "                    while len(rate_list) < max_length:\n",
    "                        rate_list.append(0.0)\n",
    "                    while len(count_list) < max_length:\n",
    "                        count_list.append(\"0\")\n",
    "                    while len(shop_list) < max_length:\n",
    "                        shop_list.append(\"\")\n",
    "                    while len(shop_url_list) < max_length:\n",
    "                        shop_url_list.append(\"\")\n",
    "\n",
    "                    for i in range(min(20, len(shop_list))):\n",
    "                        # 店名が空の場合はスキップ\n",
    "                        if not shop_list[i]:\n",
    "                            continue\n",
    "                            \n",
    "                        # URLから店舗IDを抽出\n",
    "                        if shop_url_list[i]:\n",
    "                            # URLの最後の数字部分を店舗IDとして抽出\n",
    "                            # 例: https://tabelog.com/ishikawa/A1702/A170203/17000700/ -> 17000700\n",
    "                            url_parts = shop_url_list[i].strip('/').split('/')\n",
    "                            for part in reversed(url_parts):\n",
    "                                if part.isdigit() and len(part) >= 7:  # 7桁以上の数字を店舗IDとして判定\n",
    "                                    self.shop_id = part\n",
    "                                    break\n",
    "                            else:\n",
    "                                # 数字が見つからない場合は連番\n",
    "                                self.shop_id = str(self.id_num).zfill(5)\n",
    "                        else:\n",
    "                            # URLがない場合は連番\n",
    "                            self.shop_id = str(self.id_num).zfill(5)\n",
    "                        \n",
    "                        # ジャンル情報の処理（空でない場合のみ分割）\n",
    "                        if genre_info[i]:\n",
    "                            genre_parts = genre_info[i].split(\"/\")\n",
    "                            genre = genre_parts[-1].strip() if genre_parts else \"\"\n",
    "                        else:\n",
    "                            genre = \"\"\n",
    "\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        # 店舗詳細ページから詳細情報を取得（URLが存在する場合のみ）\n",
    "                        if shop_url_list[i]:\n",
    "                            shop_detail_url = shop_url_list[i]\n",
    "                            print(f\"店舗詳細URL: {shop_detail_url}\")\n",
    "                            detail_response = self.connect_url(shop_detail_url)\n",
    "                            detail_soup = BeautifulSoup(detail_response.text, 'lxml')\n",
    "                            \n",
    "                            # 各種詳細情報を取得\n",
    "                            address = self.extract_address(detail_soup)\n",
    "                            business_hours = self.extract_business_hours(detail_soup)\n",
    "                            reserve = self.extract_reserve(detail_soup)\n",
    "                            budget_info = self.extract_budget_info(detail_soup)\n",
    "                            dinner_budget = budget_info.get('dinner', '')\n",
    "                            lunch_budget = budget_info.get('lunch', '')\n",
    "                            opened = self.extract_opened(detail_soup)\n",
    "                            # 地図情報を取得\n",
    "                            map_info = self.extract_map_info(detail_soup)\n",
    "                            latitude = map_info.get('latitude', '')\n",
    "                            longitude = map_info.get('longitude', '')\n",
    "                        else:\n",
    "                            # URLがない場合は詳細情報は空\n",
    "                            shop_detail_url = \"\"\n",
    "                            address = \"\"\n",
    "                            business_hours = \"\"\n",
    "                            reserve = \"\"\n",
    "                            dinner_budget = \"\"\n",
    "                            lunch_budget = \"\"\n",
    "                            opened = \"\"\n",
    "                            map_link = \"\"\n",
    "                            latitude = \"\"\n",
    "                            longitude = \"\"\n",
    "\n",
    "                        # データフレームに店舗詳細情報を追加（地図情報も含める）\n",
    "                        self.add_store_df(shop_list[i], genre, rate_list[i], \n",
    "                                        count_list[i], shop_detail_url, address, latitude, longitude,\n",
    "                                        business_hours, reserve, dinner_budget, lunch_budget, opened)\n",
    "                        \n",
    "                        # 結果をログ出力\n",
    "                        self.write_store_result(shop_list[i], genre_info[i].strip() if genre_info[i] else \"\", \n",
    "                                              rate_list[i], count_list[i])\n",
    "                        process_time = time.time() - start_time\n",
    "                        print('店舗ID: {}, {}件目完了：　処理時間：{:.3f}秒'.format(self.shop_id, self.id_num, process_time))\n",
    "                        self.id_num += 1\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(e)\n",
    "        except requests.exceptions.ConnectTimeout as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            end_time = time.time() - first_time\n",
    "            print('店舗詳細取得終了、処理時間：{:.3f}秒'.format(end_time))\n",
    "\n",
    "    def extract_address(self, soup):\n",
    "        \"\"\"BeautifulSoupオブジェクトから住所情報を抽出する関数\"\"\"\n",
    "        try:\n",
    "            address_element = soup.find('p', class_='rstinfo-table__address')\n",
    "            if not address_element:\n",
    "                return \"\"\n",
    "            full_address = address_element.get_text(separator=\"/\", strip=True)\n",
    "            return full_address\n",
    "        except Exception as e:\n",
    "            print(f\"住所抽出エラー: {e}\")\n",
    "            return \"\"\n",
    "            \n",
    "    def extract_business_hours(self, soup):\n",
    "        \"\"\"BeautifulSoupオブジェクトから営業時間情報を抽出する関数\"\"\"\n",
    "        try:\n",
    "            business_lists = soup.find_all('ul', class_='rstinfo-table__business-list')\n",
    "            if not business_lists:\n",
    "                return \"\"\n",
    "            \n",
    "            all_text_parts = []\n",
    "            for ul in business_lists:\n",
    "                text = ul.get_text(separator='/', strip=True)\n",
    "                if text:\n",
    "                    text = re.sub(r'\\s+', ' ', text)\n",
    "                    text = text.strip()\n",
    "                    if text:\n",
    "                        all_text_parts.append(text)\n",
    "            \n",
    "            result = ' '.join(all_text_parts)\n",
    "            result = re.sub(r'\\s+', ' ', result)\n",
    "            result = result.strip()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"営業時間抽出エラー: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_reserve(self, soup):\n",
    "        \"\"\"BeautifulSoupオブジェクトから予約可否情報を抽出する関数\"\"\"\n",
    "        try:\n",
    "            reserve_element = soup.find('p', class_='rstinfo-table__reserve-status')\n",
    "            if not reserve_element:\n",
    "                return \"\"\n",
    "            reserve = reserve_element.get_text(strip=True)\n",
    "            return reserve\n",
    "        except Exception as e:\n",
    "            print(f\"予約可否抽出エラー: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_budget_info(self, soup):\n",
    "        \"\"\"BeautifulSoupオブジェクトから予算情報を抽出する関数\"\"\"\n",
    "        try:\n",
    "            budget_div = soup.find('div', class_='rdheader-budget')\n",
    "            if not budget_div:\n",
    "                return {'dinner': '', 'lunch': ''}\n",
    "            \n",
    "            budget_info = {'dinner': '', 'lunch': ''}\n",
    "            budget_items = budget_div.find_all('p', class_='c-rating-v3')\n",
    "            \n",
    "            for item in budget_items:\n",
    "                time_icon = item.find('i', class_=re.compile(r'c-rating-v3__time'))\n",
    "                if time_icon:\n",
    "                    if 'c-rating-v3__time--dinner' in time_icon.get('class', []):\n",
    "                        time_type = 'dinner'\n",
    "                    elif 'c-rating-v3__time--lunch' in time_icon.get('class', []):\n",
    "                        time_type = 'lunch'\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    price_link = item.find('a', class_='rdheader-budget__price-target')\n",
    "                    if price_link:\n",
    "                        budget_info[time_type] = price_link.get_text(strip=True)\n",
    "                    else:\n",
    "                        price_span = item.find('span', class_='c-rating-v3__val')\n",
    "                        if price_span:\n",
    "                            budget_info[time_type] = price_span.get_text(strip=True)\n",
    "            \n",
    "            return budget_info\n",
    "        except Exception as e:\n",
    "            print(f\"予算情報抽出エラー: {e}\")\n",
    "            return {'dinner': '', 'lunch': ''}\n",
    "    \n",
    "    def extract_opened(self, soup):\n",
    "        \"\"\"BeautifulSoupオブジェクトからオープン日情報を抽出する関数\"\"\"\n",
    "        try:\n",
    "            opened_element = soup.find('p', class_='rstinfo-opened-date')\n",
    "            if not opened_element:\n",
    "                return \"\"\n",
    "            opened = opened_element.get_text(strip=True)\n",
    "            return opened\n",
    "        except Exception as e:\n",
    "            print(f\"オープン日抽出エラー: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_map_info(self, soup):\n",
    "        \"\"\"BeautifulSoupオブジェクトから座標情報を抽出する関数\"\"\"\n",
    "        try:\n",
    "            map_info = {'latitude': '', 'longitude': ''}\n",
    "            \n",
    "            # 地図のwrapperを探す\n",
    "            map_wrap = soup.find('div', class_='rstinfo-table__map-wrap')\n",
    "            if not map_wrap:\n",
    "                return map_info\n",
    "            \n",
    "            # Google Maps静的地図のimgタグから座標を抽出\n",
    "            img_element = map_wrap.find('img', class_='rstinfo-table__map-image')\n",
    "            if img_element:\n",
    "                # data-lazy-srcまたはsrcからGoogle Maps APIのURLを取得\n",
    "                map_url = img_element.get('data-lazy-src') or img_element.get('src')\n",
    "                if map_url:\n",
    "                    # centerパラメータから座標を抽出\n",
    "                    # URL例: https://maps.googleapis.com/maps/api/staticmap?...&center=36.569571136359684,136.6679997943676&...\n",
    "                    center_match = re.search(r'center=([0-9.-]+),([0-9.-]+)', map_url)\n",
    "                    if center_match:\n",
    "                        map_info['latitude'] = center_match.group(1)\n",
    "                        map_info['longitude'] = center_match.group(2)\n",
    "                    \n",
    "                    # markersパラメータからも座標を取得（より正確な場合がある）\n",
    "                    markers_match = re.search(r'markers=color:red\\|([0-9.-]+),([0-9.-]+)', map_url)\n",
    "                    if markers_match:\n",
    "                        map_info['latitude'] = markers_match.group(1)\n",
    "                        map_info['longitude'] = markers_match.group(2)\n",
    "            \n",
    "            return map_info\n",
    "        except Exception as e:\n",
    "            print(f\"地図情報抽出エラー: {e}\")\n",
    "            return {'latitude': '', 'longitude': ''}\n",
    "\n",
    "    def connect_url(self, target_url):\n",
    "        \"\"\"対象のURLにアクセスする関数\"\"\"\n",
    "        data = requests.get(target_url, timeout=10)\n",
    "        data.encoding = data.apparent_encoding\n",
    "        time.sleep(2)  # アクセス過多を避けるため、2秒スリープ\n",
    "\n",
    "        if data.status_code == requests.codes.ok:\n",
    "            return data\n",
    "        else:\n",
    "            data.raise_for_status()\n",
    "\n",
    "    def write_store_result(self, name, genre, rate, count):\n",
    "        \"\"\"店舗詳細情報のログを出力する関数\"\"\"\n",
    "        if not os.path.exists(self.RESULT_PATH):\n",
    "            os.makedirs(self.RESULT_PATH)\n",
    "        \n",
    "        file_path = os.path.join(self.RESULT_PATH, self.STORE_RESULT_NAME)\n",
    "        with open(file_path, mode='a', encoding='utf-8') as f:\n",
    "            f.write('店舗ID: {}, {}, {}, 評価：{}, 口コミ：{}件\\n'.format(self.shop_id, name, genre, rate, count))\n",
    "\n",
    "    def add_store_df(self, name, genre, rate, count, store_url, address, latitude, longitude,\n",
    "                     business_hours, reserve, dinner_budget, lunch_budget, opened):\n",
    "        \"\"\"店舗詳細データフレームに新しい行を追加する関数（座標情報も含める）\"\"\"\n",
    "        new_row = {\n",
    "            'store_id': self.shop_id,\n",
    "            'store_name': name,\n",
    "            'genre': genre,\n",
    "            'rate': rate,\n",
    "            'review_cnt': count,\n",
    "            'store_url': store_url,\n",
    "            'address': address,\n",
    "            'latitude': latitude,\n",
    "            'longitude': longitude,\n",
    "            'business_hours': business_hours,\n",
    "            'reserve': reserve,\n",
    "            'dinner_budget': dinner_budget,\n",
    "            'lunch_budget': lunch_budget,\n",
    "            'opened': opened\n",
    "        }\n",
    "        new_df = pd.DataFrame([new_row])\n",
    "        self.store_df = pd.concat([self.store_df, new_df], ignore_index=True)\n",
    "\n",
    "class TabelogReviewScraper:\n",
    "    \"\"\"\n",
    "    食べログから口コミ情報を取得するクラス\n",
    "    店舗詳細URLのリストを受け取って口コミを取得する\n",
    "    \"\"\"\n",
    "    def __init__(self, store_urls_data, max_num=60, output_individual_csv=True, output_combined_csv=True):\n",
    "        \"\"\"\n",
    "        店舗URLリストを受け取って口コミスクレイピングを開始する\n",
    "        \n",
    "        Args:\n",
    "            store_urls_data: 店舗情報を含むDataFrameまたは辞書のリスト\n",
    "                           必要な列: store_id, store_name, store_url, genre, rate, review_cnt\n",
    "            output_individual_csv: 店舗ごとの個別CSVを出力するか（デフォルト: True）\n",
    "            output_combined_csv: 全店舗統合CSVを出力するか（デフォルト: True）\n",
    "        \"\"\"\n",
    "        # 取得したデータ一覧の保存場所\n",
    "        self.RESULT_PATH = 'out'\n",
    "        # 店舗ごとの口コミCSV保存場所\n",
    "        self.INDIVIDUAL_CSV_PATH = os.path.join(self.RESULT_PATH, 'reviews_by_store')\n",
    "        # 取得したデータ一覧名\n",
    "        self.REVIEW_RESULT_NAME = 'reviews.csv'\n",
    "        # 最大表示ページ数(1,200[20*60]件以降は表示不可)\n",
    "        self.max_num = max_num\n",
    "        # 口コミデータフレームの列名\n",
    "        self.REVIEW_COLUMNS = ['store_id', 'store_name', 'genre', 'rate', 'review_cnt', \n",
    "                              'user', 'date', 'rating', 'rating_detail', 'review']\n",
    "        \n",
    "        # 出力設定\n",
    "        self.output_individual_csv = output_individual_csv\n",
    "        self.output_combined_csv = output_combined_csv\n",
    "        \n",
    "        self.review_df = pd.DataFrame(columns=self.REVIEW_COLUMNS)\n",
    "        \n",
    "        # DataFrameの場合は辞書のリストに変換\n",
    "        if isinstance(store_urls_data, pd.DataFrame):\n",
    "            self.store_data = store_urls_data.to_dict('records')\n",
    "        else:\n",
    "            self.store_data = store_urls_data\n",
    "        \n",
    "        # 出力ディレクトリを作成\n",
    "        self._create_output_directories()\n",
    "        \n",
    "        self._scrape_reviews()\n",
    "\n",
    "    def _create_output_directories(self):\n",
    "        \"\"\"出力ディレクトリを作成する\"\"\"\n",
    "        if not os.path.exists(self.RESULT_PATH):\n",
    "            os.makedirs(self.RESULT_PATH)\n",
    "        \n",
    "        if self.output_individual_csv and not os.path.exists(self.INDIVIDUAL_CSV_PATH):\n",
    "            os.makedirs(self.INDIVIDUAL_CSV_PATH)\n",
    "\n",
    "    def _scrape_reviews(self):\n",
    "        \"\"\"口コミ情報をスクレイピングするメソッド\"\"\"\n",
    "        try:\n",
    "            first_time = time.time()\n",
    "            \n",
    "            for store_info in self.store_data:\n",
    "                start_time = time.time()\n",
    "                store_id = store_info['store_id']\n",
    "                store_name = store_info['store_name']\n",
    "                genre = store_info['genre']\n",
    "                rate = store_info['rate']\n",
    "                review_cnt = store_info['review_cnt']\n",
    "                store_url = store_info['store_url']\n",
    "                \n",
    "                print(f\"口コミ取得開始: {store_name} (ID: {store_id})\")\n",
    "                \n",
    "                # 店舗ごとの口コミDataFrameを初期化\n",
    "                store_review_df = pd.DataFrame(columns=self.REVIEW_COLUMNS)\n",
    "                \n",
    "                # ここで review_count を初期化\n",
    "                review_count = 0  # 実際に取得できた口コミ数をカウント\n",
    "                \n",
    "                for page_num in range(1, self.max_num):\n",
    "\n",
    "                    review_url = store_url + 'dtlrvwlst/COND-0/smp1/D-visit/' + str(page_num) + '/?smp=1&lc=0&rvw_part=all'\n",
    "                    \n",
    "                    response = self.connect_url(review_url)\n",
    "                    # HTML解析用の変数\n",
    "                    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                    # エラーページかどうか\n",
    "                    error_element = soup.find('h2', class_='error-common__title')\n",
    "                    error = error_element.get_text(strip=True) if error_element else '0'\n",
    "                    # エラーページの場合はbreak\n",
    "                    if error == 'お探しのページが見つかりません':\n",
    "                        break                    \n",
    "\n",
    "                    print(f\"口コミURL: {review_url}\")\n",
    "                    review_url_list = soup.find_all('div', class_='rvw-item js-rvw-item-clickable-area')\n",
    "    \n",
    "                    # 各口コミページに遷移し、最新の口コミを取得する\n",
    "                    review_count = 0  # 実際に取得できた口コミ数をカウント\n",
    "                    \n",
    "                    for url in review_url_list:\n",
    "                        try:\n",
    "                            # 正しいセレクタで詳細URLを取得\n",
    "                            detail_link = url.find('a', class_='c-link-circle js-link-bookmark-detail')\n",
    "                            if detail_link is None:\n",
    "                                print(f\"詳細リンクが見つかりません\")\n",
    "                                continue\n",
    "                                \n",
    "                            detail_url = detail_link.get('data-detail-url')\n",
    "                            if detail_url is None:\n",
    "                                print(f\"data-detail-url属性が見つかりません\")\n",
    "                                continue\n",
    "                            \n",
    "                            review_detail_url = 'https://tabelog.com' + detail_url\n",
    "                            response = self.connect_url(review_detail_url)\n",
    "                            soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "                            # 口コミ投稿者\n",
    "                            user_element = soup.find('a', class_='rvw-item__rvwr-name')\n",
    "                            user = user_element.get_text(strip=True) if user_element else \"\"\n",
    "    \n",
    "                            # 口コミ訪問月\n",
    "                            date_element = soup.find('div', class_='rvw-item__single-date')\n",
    "                            date = date_element.get_text(separator=\" \", strip=True) if date_element else \"\"\n",
    "                            \n",
    "                            # 口コミ評価点\n",
    "                            rating_element = soup.find('b', class_='c-rating-v3__val c-rating-v3__val--strong')\n",
    "                            rating = rating_element.get_text(strip=True) if rating_element else \"\"\n",
    "                            \n",
    "                            # 口コミ評価点(詳細)\n",
    "                            rating_detail_element = soup.find('ul', class_='c-rating-detail')\n",
    "                            rating_detail = rating_detail_element.get_text(separator=\"/\", strip=True) if rating_detail_element else \"\"\n",
    "                            \n",
    "                            # 口コミテキストを取得（見つからない場合は空文字列）\n",
    "                            review_text = \"\"\n",
    "                            # パターン1: rvw-item__rvw-comment内のpタグ\n",
    "                            review_elements = soup.find_all('div', class_='rvw-item__rvw-comment rvw-item__rvw-comment--custom')\n",
    "                            if review_elements:\n",
    "                                for elem in review_elements:\n",
    "                                    p_tag = elem.find('p')\n",
    "                                    if p_tag and p_tag.text.strip():\n",
    "                                        review_text = p_tag.text.strip()\n",
    "                                        break\n",
    "                            # パターン2: 他の可能性のあるセレクタ\n",
    "                            if not review_text:\n",
    "                                review_elements = soup.find_all('div', class_='rvw-item__rvw-comment--custom')\n",
    "                                if review_elements:\n",
    "                                    for elem in review_elements:\n",
    "                                        p_tag = elem.find('p')\n",
    "                                        if p_tag and p_tag.text.strip():\n",
    "                                            review_text = p_tag.text.strip()\n",
    "                                            break\n",
    "\n",
    "                            # 口コミテキストがない場合でもログ出力\n",
    "                            if not review_text:\n",
    "                                print(f\"口コミテキストが見つかりません（空欄として記録）: {review_detail_url}\")\n",
    "\n",
    "                            # 全体のデータフレームに格納（統合CSV用）\n",
    "                            if self.output_combined_csv:\n",
    "                                self.add_review_df(store_id, store_name, genre, rate, review_cnt,\n",
    "                                                 user, date, rating, rating_detail, review_text)\n",
    "                            \n",
    "                            # 店舗ごとのデータフレームに格納（個別CSV用）\n",
    "                            if self.output_individual_csv:\n",
    "                                store_review_df = self.add_store_review_df(store_review_df, store_id, store_name, genre, rate, review_cnt,\n",
    "                                                       user, date, rating, rating_detail, review_text)\n",
    "                            \n",
    "                            review_count += 1\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"口コミ取得中にエラー: {e}\")\n",
    "                            continue\n",
    "                \n",
    "                # 店舗ごとのCSVファイルを出力\n",
    "                if self.output_individual_csv and len(store_review_df) > 0:\n",
    "                    self.save_store_review_csv(store_id, store_name, store_review_df)\n",
    "                \n",
    "                # 結果をログ出力\n",
    "                if review_count > 0:\n",
    "                    self.write_review_result(store_name, genre, rate, review_cnt, review_count)\n",
    "                    process_time = time.time() - start_time\n",
    "                    print('{}完了：　処理時間：{:.3f}秒, 口コミ取得数：{}件'.format(store_name, process_time, review_count))\n",
    "                else:\n",
    "                    print(f\"店舗「{store_name}」：口コミが取得できませんでした\")\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(e)\n",
    "        except requests.exceptions.ConnectTimeout as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            end_time = time.time() - first_time\n",
    "            print('口コミ取得終了、処理時間：{:.3f}秒'.format(end_time))\n",
    "            \n",
    "            # 統合CSVファイルを出力\n",
    "            if self.output_combined_csv and len(self.review_df) > 0:\n",
    "                combined_csv_path = os.path.join(self.RESULT_PATH, 'reviews_combined.csv')\n",
    "                self.review_df.to_csv(combined_csv_path, index=False, encoding='utf-8')\n",
    "                print(f\"統合CSV出力完了: {combined_csv_path}\")\n",
    "\n",
    "    def connect_url(self, target_url):\n",
    "        \"\"\"対象のURLにアクセスする関数\"\"\"\n",
    "        data = requests.get(target_url, timeout=10)\n",
    "        data.encoding = data.apparent_encoding\n",
    "        time.sleep(2)  # アクセス過多を避けるため、2秒スリープ\n",
    "\n",
    "        if data.status_code == requests.codes.ok:\n",
    "            return data\n",
    "        else:\n",
    "            data.raise_for_status()\n",
    "\n",
    "    def write_review_result(self, name, genre, rate, count, review_count):\n",
    "        \"\"\"口コミ取得結果のログを出力する関数\"\"\"\n",
    "        if not os.path.exists(self.RESULT_PATH):\n",
    "            os.makedirs(self.RESULT_PATH)\n",
    "            \n",
    "        file_path = os.path.join(self.RESULT_PATH, self.REVIEW_RESULT_NAME)\n",
    "        with open(file_path, mode='a', encoding='utf-8') as f:\n",
    "            f.write('{}, {}, 評価：{}, 口コミ：{}件, 取得口コミ：{}件\\n'.format(name, genre, rate, count, review_count))\n",
    "\n",
    "    def add_review_df(self, store_id, store_name, genre, rate, review_cnt, user, date, rating, rating_detail, comment):\n",
    "        \"\"\"全体の口コミデータフレームに新しい行を追加する関数（統合CSV用）\"\"\"\n",
    "        new_row = {\n",
    "            'store_id': store_id,\n",
    "            'store_name': store_name,\n",
    "            'genre': genre,\n",
    "            'rate': rate,\n",
    "            'review_cnt': review_cnt,\n",
    "            'user': user,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'rating_detail': rating_detail, \n",
    "            'review': comment\n",
    "        }\n",
    "        new_df = pd.DataFrame([new_row])\n",
    "        self.review_df = pd.concat([self.review_df, new_df], ignore_index=True)\n",
    "        \n",
    "    def add_store_review_df(self, store_df, store_id, store_name, genre, rate, review_cnt, user, date, rating, rating_detail, comment):\n",
    "        \"\"\"店舗ごとの口コミデータフレームに新しい行を追加する関数（個別CSV用）\"\"\"\n",
    "        new_row = {\n",
    "            'store_id': store_id,\n",
    "            'store_name': store_name,\n",
    "            'genre': genre,\n",
    "            'rate': rate,\n",
    "            'review_cnt': review_cnt,\n",
    "            'user': user,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'rating_detail': rating_detail, \n",
    "            'review': comment\n",
    "        }\n",
    "        new_df = pd.DataFrame([new_row])\n",
    "        # 元のDataFrameに直接追加（参照を変更）\n",
    "        updated_df = pd.concat([store_df, new_df], ignore_index=True)\n",
    "        return updated_df\n",
    "\n",
    "    def save_store_review_csv(self, store_id, store_name, store_df):\n",
    "        \"\"\"店舗ごとの口コミCSVファイルを保存する関数\"\"\"\n",
    "        if len(store_df) > 0:\n",
    "            # ファイル名に使えない文字を置換\n",
    "            safe_name = store_name.replace('/', '_').replace('\\\\', '_').replace(':', '_')\n",
    "            filename = f\"{store_id}_{safe_name}_reviews.csv\"\n",
    "            filepath = os.path.join(self.INDIVIDUAL_CSV_PATH, filename)\n",
    "            store_df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "            print(f\"個別CSV出力完了: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd84aa0",
   "metadata": {},
   "source": [
    "### 対象市町村に含まれる町丁目リストの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84025ecc-6282-42ea-888b-0a7bed518daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':   \n",
    "    # 市町村別URLの入力\n",
    "    df = pd.read_csv('in/Ishikawa_city_url.csv')  # 市町村URLリスト\n",
    "    df = df[df['city'] == \"金沢市\"]  # 特定の市町村を対象とする場合はここで絞る\n",
    "    \n",
    "    # 町別URLへの変換\n",
    "    extractor = TownUrlExtractor() \n",
    "    towns = extractor.process_dataframe(df, url_column='url')\n",
    "    \n",
    "    # 出力させる場合\n",
    "    # extractor.save_results(towns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ddd37",
   "metadata": {},
   "source": [
    "### 各町丁目に含まれるすべての店舗の詳細情報の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b44c7-cbea-45f2-8890-5a63862feca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    store_scraper = TabelogStoreScraper(url_list=towns)\n",
    "    store_scraper.store_df.to_csv('out/store_details.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cfa25",
   "metadata": {},
   "source": [
    "### 各店舗における口コミ情報の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5a637-16c2-460b-ad5d-ec7658b34597",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    review_scraper_individual_only = TabelogReviewScraper(\n",
    "        store_scraper.store_df,      # 店舗詳細出力のdataframeを読み込む \n",
    "        max_num = 2,                 # 40件まで読み込む\n",
    "        output_individual_csv=True,  # 店舗ごとのCSVを出力\n",
    "        output_combined_csv=False    # 統合CSVは出力しない\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c167b9-56a1-436a-a232-567450ed3a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0298ef-0498-492b-a765-1870432a4cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f85aad-3ff8-4267-aa00-ddf6a0075c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
